{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "AxZ4Co5xcehO",
        "outputId": "4c559f0c-7a71-4f0a-beee-ca28d0a46968"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2f828325-2aca-4b31-8b51-85a503e86946\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2f828325-2aca-4b31-8b51-85a503e86946\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving christchatbot.json to christchatbot.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "d=files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_json('christchatbot.json')"
      ],
      "metadata": {
        "id": "cI5rpeHYcpeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stem(word):\n",
        "  return stemmer.stem(word.lower())\n",
        "\n",
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "  tokenized_sentence=[stem(w) for w in tokenized_sentence]\n",
        "  # this line will stem the words to their root form\n",
        "  bag=np.zeros(len(all_words), dtype=np.float32)\n",
        "  # in the above line we are creating a vector which is filled by zero\n",
        "  # the length of vector is equal to number of words in all_words\n",
        "  for idx , w in enumerate(all_words):\n",
        "    if w in tokenized_sentence:\n",
        "      bag[idx]=1.0\n",
        "  # if the current word is in the tokenized sentence , then the corressponding feature of bag of words is 1.0\n",
        "\n",
        "\n",
        "  return bag\n",
        "\n",
        "\n",
        "sentence=[\"hello\", \"how\", \"are\", \"you\"]\n",
        "words=[\"hi\", \"hello\",\"I\", \"you\",\"bye\",\"thank\",\"cool\"]\n",
        "bag=bag_of_words(sentence, words)\n",
        "print(bag)\n",
        "\n",
        "a=\"how are you \"\n",
        "a=tokenize(a)\n",
        "words=['organize','organizes', 'organizing ']\n",
        "stemmed_words=[stem(x) for x in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8dSIr1NodUe",
        "outputId": "541e5280-dc8a-4734-c326-8cc302a5128e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 0. 1. 0. 0. 0.]\n",
            "['organ', 'organ', 'organizing ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)\n",
        "import torch\n",
        "# torch refers to the pytorch library\n",
        "# it is developed by facebook ai research lab(fair)\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOZLF_KXYCZ0",
        "outputId": "7bc6f842-d660-4d70-b0b1-014dcf1a7739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              intents\n",
            "0   {'tag': 'greeting', 'patterns': ['Hi', 'Hey', ...\n",
            "1   {'tag': 'blocks', 'patterns': ['how many build...\n",
            "2   {'tag': 'hostel', 'patterns': ['do you have ho...\n",
            "3   {'tag': 'goodbye', 'patterns': ['Bye', 'See yo...\n",
            "4   {'tag': 'thanks', 'patterns': ['Thanks', 'Than...\n",
            "5   {'tag': 'items', 'patterns': ['Which items do ...\n",
            "6   {'tag': 'food', 'patterns': ['I am hungry', 'w...\n",
            "7   {'tag': 'central block', 'patterns': ['i am st...\n",
            "8   {'tag': 'payments', 'patterns': ['Do you take ...\n",
            "9   {'tag': 'delivery', 'patterns': ['How long doe...\n",
            "10  {'tag': 'funny', 'patterns': ['Tell me a joke!...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# we are using a context manager \"with\"\n",
        "# context manaager allow us to read the json file in read only mode\n",
        "# it allow us to close the file even when an error occurs\n",
        "with open('christchatbot.json','r') as f:\n",
        "  intents=json.load(f)"
      ],
      "metadata": {
        "id": "vJnTxTSipbv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#intent is like a goal or category, it is the main thing a person is trying to do when they talk to chatbot\n",
        "all_words=[]\n",
        "tags=[]\n",
        "xy=[]\n",
        "for intent in intents['intents']:\n",
        "  tag=intent['tag']\n",
        "  # here we are trying to extent the tag or label of that intent\n",
        "  tags.append(tag)\n",
        "  for pattern in intent['patterns']:\n",
        "    # we are performing tokenize operation on all sentence of pattern\n",
        "    # the result is stored in w variable\n",
        "    w=tokenize(pattern)\n",
        "\n",
        "    all_words.extend(w)\n",
        "    xy.append((w,tag))\n",
        "    # here we are storing words with their corresponding intent tags\n",
        "\n",
        "ignore_words=['?','!','.',',']\n",
        "all_words=[stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words=sorted(set(all_words))\n",
        "# in the above code we are sorting the variable based on alphabetical order and removing duplicates\n",
        "\n",
        "tags=sorted(set(tags))\n",
        "print(tags)\n",
        "print(all_words)\n",
        "print(xy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q2I9NLhpean",
        "outputId": "6b6de98d-d815-4ba7-812f-06ce3b9033ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['blocks', 'central block', 'delivery', 'food', 'funny', 'goodbye', 'greeting', 'hostel', 'items', 'payments', 'thanks']\n",
            "[\"'s\", '1', '4', '5', 'a', 'accept', 'am', 'and', 'ani', 'anyon', 'are', 'block', 'boy', 'build', 'bye', 'can', 'canteen', 'card', 'cash', 'central', 'credit', 'day', 'deliveri', 'do', 'doe', 'eat', 'food', 'for', 'front', 'funni', 'gate', 'get', 'girl', 'go', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'hostel', 'how', 'hungri', 'i', 'in', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mani', 'mastercard', 'me', 'my', 'name', 'nearest', 'no', 'of', 'onli', 'option', 'pay', 'paypal', 'see', 'sell', 'ship', 'snack', 'someth', 'stand', 'take', 'tell', 'thank', 'that', 'the', 'there', 'to', 'want', 'what', 'when', 'where', 'which', 'with', 'you', 'your']\n",
            "[(['Hi'], 'greeting'), (['Hey'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting'), (['how', 'many', 'buildings', 'are', 'there'], 'blocks'), (['how', 'many', 'blocks', 'are', 'there'], 'blocks'), (['do', 'you', 'have', 'hostel'], 'hostel'), (['what', 'is', 'the', 'name', 'of', 'boys', 'hostel'], 'hostel'), (['what', 'is', 'the', 'name', 'of', 'girls', 'hostel'], 'hostel'), (['where', 'is', 'the', 'boys', 'hostel'], 'hostel'), (['where', 'is', 'the', 'girls', 'hostel'], 'hostel'), (['Bye'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['Thank', 'for', 'your', 'help'], 'goodbye'), (['Thanks'], 'thanks'), (['Thank', 'you'], 'thanks'), (['That', \"'s\", 'helpful'], 'thanks'), (['Thank', \"'s\", 'a', 'lot', '!'], 'thanks'), (['thank', 'you', 'for', 'your', 'help'], 'thanks'), (['Which', 'items', 'do', 'you', 'have', '?'], 'items'), (['What', 'kinds', 'of', 'items', 'are', 'there', '?'], 'items'), (['What', 'do', 'you', 'sell', '?'], 'items'), (['I', 'am', 'hungry'], 'food'), (['what', 'are', 'the', 'options', 'for', 'food'], 'food'), (['i', 'want', 'to', 'have', 'something', 'to', 'eat'], 'food'), (['where', 'is', 'the', 'nearest', 'food', 'option'], 'food'), (['is', 'there', 'any', 'canteen'], 'food'), (['i', 'want', 'to', 'have', 'a', 'snack'], 'food'), (['i', 'am', 'standing', 'in', 'front', 'of', 'gate', 'no', '1', 'and', 'i', 'want', 'to', 'go', 'to', 'central', 'block'], 'central block'), (['i', 'am', 'standing', 'in', 'front', 'of', 'gate', 'no', '4', 'and', 'i', 'want', 'to', 'go', 'to', 'central', 'block'], 'central block'), (['i', 'am', 'standing', 'in', 'front', 'of', 'gate', 'no', '5', 'and', 'i', 'want', 'to', 'go', 'to', 'central', 'block'], 'central block'), (['Do', 'you', 'take', 'credit', 'cards', '?'], 'payments'), (['Do', 'you', 'accept', 'Mastercard', '?'], 'payments'), (['Can', 'I', 'pay', 'with', 'Paypal', '?'], 'payments'), (['Are', 'you', 'cash', 'only', '?'], 'payments'), (['How', 'long', 'does', 'delivery', 'take', '?'], 'delivery'), (['How', 'long', 'does', 'shipping', 'take', '?'], 'delivery'), (['When', 'do', 'I', 'get', 'my', 'delivery', '?'], 'delivery'), (['Tell', 'me', 'a', 'joke', '!'], 'funny'), (['Tell', 'me', 'something', 'funny', '!'], 'funny'), (['Do', 'you', 'know', 'a', 'joke', '?'], 'funny')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# code to create training dataset for a chatbot using bag of words\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "for (pattern_sentence, tag) in xy:\n",
        "  bag=bag_of_words(pattern_sentence, all_words)\n",
        "  # print(\"this is the bad\",bag)\n",
        "  x_train.append(bag)\n",
        "\n",
        "  label=tags.index(tag)\n",
        "\n",
        "  # print(\"this is the label\",label)\n",
        "  y_train.append(label)\n",
        "\n",
        "x_train=np.array(x_train)\n",
        "\n",
        "\n",
        "# vocabulary is:\n",
        "# [\"cat\", \"dog\", \"bird\"]\n",
        "#  pattern sentence is:\n",
        "# \"The cat sat on the mat.\"\n",
        "#  bag-of-words representation of this pattern sentence would be the following vector:\n",
        "# [1, 0, 0]"
      ],
      "metadata": {
        "id": "hfyCKsyUr8Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset is a predeined class in Pytorch used for creating custom datasets for machine learning tasks\n",
        "# chatdataset is a custom clsas that inherits pytorch dataset class\n",
        "class ChatDataset(Dataset):\n",
        "  # here we have implemented three methods of dataset class which are\n",
        "  # a) __init__\n",
        "  # b) __getitem__\n",
        "  # c) __len__\n",
        "  def __init__(self):\n",
        "    self.n_samples=len(x_train)\n",
        "    self.x_data=x_train\n",
        "    self.y_data=y_train\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # this method will return the data and it's label based on index\n",
        "    return self.x_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    # return total number of sample in data\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "batch_size=8\n",
        "# model will receive 8 samples at a time to update its weights\n",
        "dataset=ChatDataset()\n",
        "# creating instance of chatdataset class\n",
        "train_loader=DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# data loader is a utility function that help us to efficiently load and manager out data for trainig  machine learning models/deep learming model\n",
        "# it is part of torch.utils.data\n",
        "# shuffle is true allow data loader to shuffle the datatset at the beginning of each dataset\n",
        "# num_workers will control how many worker processes are used to load data in parallel during training"
      ],
      "metadata": {
        "id": "PW0WuPfXr9xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# in this network we have used three layers\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  # NeuralNet is a custom class\n",
        "  # this will inherit a class called nn.Module\n",
        "  # nn.Module is a base class for all Pytorch neural network\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    # the above function is a constructor which take three parameters\n",
        "    # input size - no of input features\n",
        "    # hidden size is no of neurons for hidden layers\n",
        "    # output layer is no of output classses\n",
        "    super(NeuralNet, self).__init__()\n",
        "    # network architecture\n",
        "    self.l1=nn.Linear(input_size, hidden_size)\n",
        "    self.l2=nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3=nn.Linear(hidden_size, num_classes)\n",
        "     # nn.Linear - fullly connected linear layers. they specify the input and output of each layer\n",
        "    self.relu=nn.ReLU()\n",
        "    # rectified linear unit activation function applied after each linear layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    # this method will define the forward pass of the neural network /how data flows\n",
        "    out=self.l1(x) #applied first linear layer to the input x\n",
        "    out=self.relu(out) # apply relu to the output of first layer\n",
        "    out=self.l2(out) #apply second linear layer to the output of first layer\n",
        "    out=self.relu(out)\n",
        "    out=self.l3(out)\n",
        "\n",
        "    # final out represents output of the neural network\n",
        "    return out"
      ],
      "metadata": {
        "id": "sO6PFzGk5VD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=8\n",
        "hidden_size=8\n",
        "output_size=len(tags)\n",
        "# output is number of tags in our dataset\n",
        "input_size=len(x_train[0])\n",
        "learning_rate=0.001\n",
        "# learning rate is a hyper parameter\n",
        "# it determine how quickly or slowly a model learns\n",
        "# slower model can lead to stable convergence\n",
        "num_epochs=1000\n",
        "\n",
        "\n",
        "\n",
        "print(input_size, len(all_words))\n",
        "# important to check the dimesnion of input with vocabulary\n",
        "print(output_size, tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1Jm5Vyu6d47",
        "outputId": "d868168c-b569-41b3-e8eb-616eb241a7dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87 87\n",
            "11 funny\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# the above line checks if GPU is avaialble or not\n",
        "# if yes it will use it for training\n",
        "# if no cpu will be used\n",
        "model=NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "# creating an instance of Neuralnet custom class\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "# defining loss function which is cross entropy loss\n",
        "# it is mainly used for multi class classification problem\n",
        "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# adam optimizer is used to update the model parameter during traiining\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for (words, labels) in train_loader:\n",
        "    words=words.to(device)\n",
        "    labels=labels.to(device)\n",
        "    # the above two lines will move both input and output to device(gpu/cpu)\n",
        "\n",
        "\n",
        "    outputs=model(words)\n",
        "    # in above line we will pass the input to the neural network to obtain predictions\n",
        "    loss=criterion(outputs, labels)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    # the above line clear the gradient before computing new gradients during backpropagation\n",
        "    loss.backward()\n",
        "    # above line perform back propagation by computing gradient of loss with respect\n",
        "    # to the model's parameter\n",
        "    optimizer.step()\n",
        "    # the above line uses optimizer to update the model's parameter (weight and bias)\n",
        "\n",
        "\n",
        "  if(epoch+1)%100==0:\n",
        "     print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    #  we display all epcohs whose value is multiple of 100 to track the progress\n",
        "\n",
        "\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfz7IVLT9lmf",
        "outputId": "6457b1d3-a610-4be1-e60c-8d2bc3920b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 1.3713\n",
            "Epoch [200/1000], Loss: 0.2902\n",
            "Epoch [300/1000], Loss: 0.1844\n",
            "Epoch [400/1000], Loss: 0.0108\n",
            "Epoch [500/1000], Loss: 0.0052\n",
            "Epoch [600/1000], Loss: 0.0070\n",
            "Epoch [700/1000], Loss: 0.0012\n",
            "Epoch [800/1000], Loss: 0.0014\n",
            "Epoch [900/1000], Loss: 0.0002\n",
            "Epoch [1000/1000], Loss: 0.0009\n",
            "final loss: 0.0009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data={\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"input_size\": input_size,\n",
        "    \"output_size\": output_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"all_words\": all_words,\n",
        "    \"tags\": tags\n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "# created a python dictionary named data\n",
        "# model state represent the trained weights and bias of model\n",
        "# the hyperparamters (input , output and hiddent size)\n",
        "# all words represent the vocabulary\n",
        "# tags represent the labels in dataset\n",
        "\n",
        "FILE=\"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "# the variable is stored in data.pth\n",
        "\n",
        "print(f'training complete . file saved to {FILE}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFcK-BeoEKH8",
        "outputId": "46edd5ae-f64b-43a2-994b-2612160d78e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete . file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('christchatbot.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "\n",
        "FILE=\"data.pth\"\n",
        "data=torch.load(FILE)\n",
        "# loading pre trained neural network\n",
        "\n",
        "input_size=data[\"input_size\"]\n",
        "hidden_size=data[\"hidden_size\"]\n",
        "output_size=data[\"output_size\"]\n",
        "all_words=data[\"all_words\"]\n",
        "tags=data[\"tags\"]\n",
        "model_state=data[\"model_state\"]\n",
        "\n",
        "\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "# the above line initialize weight and bias from pre trained model\n",
        "\n",
        "model.eval()\n",
        "# putting model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3yepHDaE291",
        "outputId": "da42e720-018b-45e5-c9b8-0424226d6c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (l1): Linear(in_features=87, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=11, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = \"Sam\"\n",
        "print(\"Let's chat! (type 'quit' to exit)\")\n",
        "while True:\n",
        "    # sentence = \"do you use credit cards?\"\n",
        "    sentence = input(\"You: \")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "\n",
        "    sentence = tokenize(sentence)\n",
        "    X = bag_of_words(sentence, all_words)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    # the above line reshapw the bag of words to format expected by model\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "\n",
        "    output = model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = tags[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents['intents']:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "    else:\n",
        "        print(f\"{bot_name}: I do not understand...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwuH26z9F6kf",
        "outputId": "a34d05ed-e4e6-4751-c2ec-874d96f14b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'quit' to exit)\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the trained model to a file using pickle\n",
        "with open('chatbot_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)\n"
      ],
      "metadata": {
        "id": "LhVvb7gCfxx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Load the model from the pickle file\n",
        "with open('chatbot_model.pkl', 'rb') as model_file:\n",
        "    loaded_model = pickle.load(model_file)\n",
        "\n",
        "# Assuming you have the necessary functions for tokenization, bag_of_words, etc.\n",
        "\n",
        "# Function to predict using the loaded model\n",
        "def predict(sentence):\n",
        "    sentence = tokenize(sentence)\n",
        "    X = bag_of_words(sentence, all_words)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "\n",
        "    output = loaded_model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = tags[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "\n",
        "    if prob.item() > 0.75:\n",
        "        for intent in intents['intents']:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                return random.choice(intent['responses'])\n",
        "    else:\n",
        "        return \"I do not understand...\"\n",
        "\n",
        "\n",
        "\n",
        "bot_name = \"Sam\"\n",
        "print(f\"Bot: Hi! I'm {bot_name}. Type 'quit' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        print(f\"Bot: Goodbye! Have a great day!\")\n",
        "        break\n",
        "\n",
        "    response = predict(user_input)\n",
        "    print(f\"Bot: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gahdYH-fgVh5",
        "outputId": "2a939b8b-7424-435b-ec3e-e1c3179f870b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Hi! I'm Sam. Type 'quit' to exit.\n",
            "You: quit\n",
            "Bot: Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming your loaded_model is on cuda:0\n",
        "loaded_model.to('cpu')\n",
        "\n",
        "# Create a dummy input tensor on the same device (CPU in this case)\n",
        "input_tensor = torch.zeros(1, input_size).to('cpu')\n",
        "\n",
        "# Convert the model to TorchScript\n",
        "torchscript_model = torch.jit.trace(loaded_model, input_tensor)\n",
        "torchscript_model.save('chatbot_model.pt')\n"
      ],
      "metadata": {
        "id": "P13eKsdXtawF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9WV1rqjwIWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}